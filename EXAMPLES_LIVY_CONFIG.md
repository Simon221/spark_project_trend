# Exemples d'utilisation: Configuration Livy avanc√©e

## üìå Cas d'usage 1: JAR avec d√©pendances suppl√©mentaires

**Sc√©nario**: Votre JAR de rattrapage a besoin de d√©pendances externes (Apache Commons Lang, Jackson, etc.)

### Configuration .env
```env
LIVY_JARS=hdfs:///user/sddesigner/lib/commons-lang3-3.12.0.jar,hdfs:///user/sddesigner/lib/jackson-databind-2.15.0.jar,hdfs:///user/sddesigner/lib/httpcomponents-client-4.5.jar
LIVY_FILES=
LIVY_ARCHIVES=
LIVY_PY_FILES=
LIVY_CONF={}
```

### R√©sultat
- Toutes les JARs sont ajout√©es au classpath Spark
- Votre code Java peut importer et utiliser ces libraires
- Utile pour √©viter "ClassNotFoundException"

---

## üìå Cas d'usage 2: Configuration Spark avanc√©e

**Sc√©nario**: Vous avez besoin de tuner les performances (shuffle partitions, allocation dynamique, etc.)

### Configuration .env
```env
LIVY_JARS=
LIVY_FILES=
LIVY_ARCHIVES=
LIVY_PY_FILES=
LIVY_CONF={
  "spark.sql.shuffle.partitions": "400",
  "spark.dynamicAllocation.enabled": "true",
  "spark.dynamicAllocation.minExecutors": "2",
  "spark.dynamicAllocation.maxExecutors": "20",
  "spark.shuffle.compress": "true",
  "spark.executor.heartbeatInterval": "60s",
  "spark.sql.adaptive.enabled": "true",
  "spark.broadcast.blockSize": "128m"
}
```

### R√©sultat
- Spark sera ex√©cut√© avec cette configuration
- Am√©lioration des performances pour les op√©rations group√©es
- Utile pour g√©rer la m√©moire et les ressources

---

## üìå Cas d'usage 3: Distribution de fichiers de configuration

**Sc√©nario**: Votre JAR a besoin de fichiers de configuration (properties, XML, logs, etc.)

### Configuration .env
```env
LIVY_JARS=
LIVY_FILES=hdfs:///config/app.properties,hdfs:///config/log4j.xml,hdfs:///sql/recovery-queries.sql,hdfs:///data/static-lookup.csv
LIVY_ARCHIVES=
LIVY_PY_FILES=
LIVY_CONF={}
```

### Structure attendue sur Spark
```
$WORK_DIR/
  ‚îú‚îÄ app.properties        (depuis LIVY_FILES)
  ‚îú‚îÄ log4j.xml             (depuis LIVY_FILES)
  ‚îú‚îÄ recovery-queries.sql  (depuis LIVY_FILES)
  ‚îî‚îÄ static-lookup.csv     (depuis LIVY_FILES)
```

### Code Java/Scala pour acc√©der
```java
// Les fichiers sont dans le r√©pertoire courant du job
FileInputStream fis = new FileInputStream("app.properties");
Properties props = new Properties();
props.load(fis);
```

### R√©sultat
- Configuration centralis√©e et versionn√©e en HDFS
- Facile √† mettre √† jour sans recompiler le JAR
- Chaque job obtient la derni√®re version

---

## üìå Cas d'usage 4: Archives avec mod√®les ou donn√©es

**Sc√©nario**: Vous avez besoin de distribuer un archive compress√©e (mod√®les ML, donn√©es statiques, etc.)

### Configuration .env
```env
LIVY_JARS=
LIVY_FILES=hdfs:///config/database.properties
LIVY_ARCHIVES=hdfs:///models/ml-models-2024.tar.gz,hdfs:///data/reference-tables.zip
LIVY_PY_FILES=
LIVY_CONF={}
```

### Structure du .tar.gz
```
ml-models-2024.tar.gz
  ‚îî‚îÄ models/
      ‚îú‚îÄ churn-model.pkl
      ‚îú‚îÄ recommendation-model.pkl
      ‚îî‚îÄ scoring-model.pkl
```

### Structure du .zip
```
reference-tables.zip
  ‚îú‚îÄ cities.csv
  ‚îú‚îÄ countries.csv
  ‚îî‚îÄ currencies.csv
```

### R√©sultat apr√®s extraction sur Spark
```
$WORK_DIR/
  ‚îú‚îÄ models/
  ‚îÇ   ‚îú‚îÄ churn-model.pkl
  ‚îÇ   ‚îú‚îÄ recommendation-model.pkl
  ‚îÇ   ‚îî‚îÄ scoring-model.pkl
  ‚îú‚îÄ cities.csv
  ‚îú‚îÄ countries.csv
  ‚îú‚îÄ currencies.csv
  ‚îî‚îÄ database.properties
```

---

## üìå Cas d'usage 5: JAR complet avec toutes les options

**Sc√©nario**: Setup production complet avec toutes les options

### Configuration .env
```env
# D√©pendances suppl√©mentaires
LIVY_JARS=hdfs:///lib/commons-lang3-3.12.0.jar,hdfs:///lib/jackson-core-2.15.0.jar,hdfs:///lib/postgresql-42.5.0.jar

# Fichiers de configuration
LIVY_FILES=hdfs:///config/app.properties,hdfs:///config/log4j.xml,hdfs:///config/database-prod.properties,hdfs:///queries/recovery.sql

# Archives compress√©es
LIVY_ARCHIVES=hdfs:///models/ml-models.tar.gz,hdfs:///data/reference-2024.zip

# Configuration Spark avanc√©e
LIVY_CONF={
  "spark.sql.shuffle.partitions": "500",
  "spark.dynamicAllocation.enabled": "true",
  "spark.dynamicAllocation.minExecutors": "4",
  "spark.dynamicAllocation.maxExecutors": "50",
  "spark.executor.memoryOverhead": "2g",
  "spark.executor.heartbeatInterval": "60s",
  "spark.sql.adaptive.enabled": "true"
}
```

### Exemple d'appel depuis le UI
```
Chemin du JAR: hdfs:///user/sddesigner/recovery-app.jar
Arguments: --date 20240115 --table splio.active --mode parallel
```

### Flux d'ex√©cution complet

1. **Payload Livy cr√©√©e avec**:
   - 3 JARs ajout√©s au classpath
   - 4 fichiers copi√©s dans le r√©pertoire courant
   - 2 archives extraites
   - Configuration Spark applic√©e

2. **Spark ex√©cute avec**:
   - 500 partitions pour les shuffles SQL
   - Jusqu'√† 50 executors (allocation dynamique)
   - 2GB de m√©moire overhead par executor
   - Optimisation adaptative des requ√™tes

3. **R√©sultat**: Job robuste et performant

---

## üìå Cas d'usage 6: Rattrapage PySpark

**Sc√©nario**: Vous avez besoin d'ex√©cuter un script PySpark avec d√©pendances Python

### Configuration .env
```env
LIVY_JARS=
LIVY_FILES=hdfs:///config/spark-config.properties,hdfs:///data/lookup-tables.csv
LIVY_ARCHIVES=hdfs:///python/packages.zip
LIVY_PY_FILES=hdfs:///python/utils.py,hdfs:///python/transformers.zip
LIVY_CONF={"spark.sql.adaptive.enabled": "true"}
```

### Structure des fichiers Python
```
python/
  ‚îú‚îÄ utils.py              (module utilitaire)
  ‚îú‚îÄ transformers.zip      (package complet)
  ‚îî‚îÄ packages.zip          (d√©pendances pip)
```

### Code du job Spark
```python
# Les fichiers sont dans le path Python
import utils
from transformers import DataTransformer

# Le package est extrait et accessible
import json
config = json.load(open("spark-config.properties"))

# Les fichiers CSV sont accessibles
df = spark.read.csv("lookup-tables.csv", header=True)
```

---

## üìù Template pour d√©marrer

Voici un template `.env` minimal pour copier-coller:

```env
# === Configuration Livy avanc√©e ===
# √Ä personnaliser selon vos besoins

# JARs √† ajouter au classpath (cha√Æne CSV)
LIVY_JARS=

# Fichiers √† copier (cha√Æne CSV)
LIVY_FILES=

# Archives √† extraire (cha√Æne CSV)
LIVY_ARCHIVES=

# Fichiers Python pour PySpark (cha√Æne CSV)
LIVY_PY_FILES=

# Configuration Spark (JSON)
# Exemples:
# - "spark.sql.shuffle.partitions": "200"
# - "spark.dynamicAllocation.enabled": "true"
# - "spark.executor.memoryOverhead": "1g"
LIVY_CONF={}
```

---

## ‚úÖ Checklist avant mise en production

- [ ] Tous les chemins HDFS sont valides (`hdfs:///...`)
- [ ] L'utilisateur `sddesigner` a acc√®s en lecture √† tous les chemins
- [ ] Les JSON dans `LIVY_CONF` sont valides (testez avec `python -m json.tool`)
- [ ] Les archives peuvent √™tre extraites sans erreur
- [ ] Les JARs correspondent √† la version Java du cluster
- [ ] Les fichiers de configuration sont √† jour
- [ ] Vous avez sauvegard√© une copie du `.env` (ne commitez PAS sur Git)
- [ ] Le serveur a √©t√© red√©marr√© apr√®s modification du `.env`
- [ ] Un test de rattrapage a √©t√© lanc√© et s'est bien ex√©cut√©

---

## üîß D√©pannage

### "ClassNotFoundException" quand le JAR s'ex√©cute

**Cause probable**: D√©pendance manquante

**Solution**: Ajoutez le JAR dans `LIVY_JARS`

```env
LIVY_JARS=hdfs:///lib/commons-lang3.jar,hdfs:///lib/missing-lib.jar
```

### "FileNotFoundException" dans le code du JAR

**Cause probable**: Fichier de configuration non distribu√©

**Solution**: Ajoutez le fichier dans `LIVY_FILES`

```env
LIVY_FILES=hdfs:///config/app.properties
```

### Job lent avec beaucoup de donn√©es

**Cause probable**: Configuration Spark sous-optimis√©e

**Solution**: Augmentez les partitions et activez l'allocation dynamique

```env
LIVY_CONF={
  "spark.sql.shuffle.partitions": "500",
  "spark.dynamicAllocation.enabled": "true",
  "spark.dynamicAllocation.maxExecutors": "30"
}
```

### Archive ne s'extrait pas correctement

**Cause probable**: Format d'archive non support√© ou chemin invalide

**V√©rifiez**: 
- L'archive existe bien (`hdfs dfs -ls hdfs:///...`)
- Format support√©: `.tar`, `.tar.gz`, `.tgz`, `.zip`
- HDFS n'a pas de probl√®me de permissions

